{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HfonHK-Z5jp6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading embedded data and"
      ],
      "metadata": {
        "id": "LfkBy37IRrax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_data = np.load(\"/content/drive/MyDrive/Colab files/Hackaton/embedded_data.npz\", allow_pickle=True)\n",
        "train_data = pd.DataFrame({\"question\": loaded_data[\"question\"], \"raw_data\": loaded_data[\"raw_data\"]})\n",
        "\n",
        "def add_mask(df_column):\n",
        "  max_length = df_column.apply(lambda x: len(x[0])).max()\n",
        "  return df_column.apply(lambda x: np.concatenate((x[0], np.array((max_length - len(x[0])) * [1024 * [0]])), axis=0) if len(x[0]) < max_length else x[0])\n",
        "\n",
        "train_data[\"question\"] = add_mask(train_data[\"question\"])\n",
        "train_data[\"raw_data\"] = add_mask(train_data[\"raw_data\"])"
      ],
      "metadata": {
        "id": "cYVB_gBA9C0L"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Transformer Block"
      ],
      "metadata": {
        "id": "fYE-F5aUS5zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 1024\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(self, num_heads, key_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
        "    self.feed_forward = keras.Sequential(\n",
        "        [layers.Dense(2048, activation=\"relu\"),\n",
        "         layers.Dense(embed_dim)]\n",
        "    )\n",
        "    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = layers.Dropout(0.1)\n",
        "    self.dropout2 = layers.Dropout(0.1)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    attention_output = self.dropout1(self.attention(inputs, inputs))\n",
        "    output1 = self.layernorm1(inputs + attention_output)\n",
        "    feed_forward_output = self.dropout2(self.feed_forward(output1))\n",
        "    return self.layernorm2(output1 + feed_forward_output)\n",
        "\n",
        "class PositionEmbedding(layers.Layer):\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.pos_emb = layers.Embedding(input_dim=self.input_dim, output_dim=embed_dim)\n",
        "\n",
        "  def call(self, X):\n",
        "    return X + self.pos_emb(np.arange(0, self.input_dim, 1))"
      ],
      "metadata": {
        "id": "HetBWFpD9-Pj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = layers.Input(shape=[None, 1024])\n",
        "embedding_layer = PositionEmbedding()(input_layer)\n",
        "x = TransformerBlock(8, 1024, 0.4)(embedding_layer)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dense(512, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "outputs = layers.Dense(32, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(input_layer, outputs)\n",
        "model.compile(loss=\"SparseCategoricalCrossentropy\", optimizer=keras.optimizers.Adam(), metrics=[\"accuracy\"])\n",
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "iZcopDCDABtS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}